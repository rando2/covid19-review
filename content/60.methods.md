## An Open-Publishing Response to the COVID-19 Infodemic

### ABSTRACT

In an effort to keep pace as new information about COVID-19 and SARS-CoV-2 becomes available, this project is an open, collaborative effort that invited contributions from the scientific community broadly, similar to previous efforts to develop collaborative reviews [@doi:10.1098/rsif.2017.0387; @url:https://greenelab.github.io/deep-review].


### CCS CONCEPTS

<!--To Do: @mprobson, do you want to take the lead on this?-->

### KEYWORDS

### INTRODUCTION

Coronavirus Disease 2019 (COVID-19) has shaped the years 2020 and 2021 by causing a world-wide public health crisis.
The scientific community has responded by turning significant attention and resources towards COVID-19 and the associated virus, SARS-CoV-2.
The result has been the rapid release of data, results, and publications related to COVID-19 at a scale never previously seen.
Over 20,000 articles about COVID-19 were released in the first 4 months of the pandemic [@doi:10.1053/j.ackd.2020.08.003], and the velocity and volume of information being released led to the pandemic being termed as an "infodemic" as well [@doi:10.1053/j.ackd.2020.08.003; @doi:10/ggpx67].
While this influx of information is likely evidence of important work towards understanding the virus and the disease, there are also downsides to the availability of too much information.
The potential for "excessive publication" has been identified as an issue for over forty years, and was one concern about the move towards electronic, rather than print, publishing at the turn of the millennium [@doi:10/d3bmnv].
The contents of the COVID-19 Open Research Dataset (CORD-19) [@arxiv:2004.10706], which was developed in part to assist in efforts to train machine learning algorithms on COVID-19-related text, illustrates the volume of publication relevant to understanding this virus (Figure X).
This resource was developed by querying several sources for terms related to SARS-CoV-2 and COVID-19, as well as the coronaviruses SARS-CoV-1 and MERS-CoV and their associated viruses [@arxiv:2004.10706].
Additional curation by CoronaCentral [@doi:10.1101/2020.12.21.423860] has produced, at present, a set of over 150,000 publications particularly relevant to COVID-19 and these closely related viruses.
Thus, any effort to synthesize, summarize, and contextualize COVID-19 research will be faced with a vast corpus of potentially relevant material.

<!--To Do: Insert CORD-19 figure & stats-->
Figure X: Change over time in the number of publications in the CORD-19 dataset.
The first release, on March 16, 2020, contained 28,000 manuscripts on topics relevant to SARS-CoV-2 and related coronaviruses [@arxiv:2004.10706].
Since then, the resource has continued to grow rapidly (left), with both traditionally published and preprint manuscripts in the corpus (right).
While not all of the manuscripts are specifically about SARS-CoV-2 or COVID-19, this corpus is likely to contain all or most manuscripts that would be explored in developing a literature review, which requires assessing both emerging and prior research.

<!--To Do: Possible viz of change in clinical trials as well?-->

Additionally, with information being produced rapidly both in traditional publishing venues and as preprints, some papers that are published face scrutiny after their initial release.
Concerns have been raised that the number of COVID-19 papers being retracted may be higher, or potentially much higher, than is typical, although a thorough investigation of this question will not be possible until more time has elapsed [@doi:10.1080/08989621.2020.1782203; @doi:10.1080/08989621.2020.1793675].
Other papers are updated with corrections or expressions of concern [@doi:10.1080/08989621.2020.1793675; @url:https://retractionwatch.com/retracted-coronavirus-covid-19-papers].
These include both preprints and papers that are published in more traditional venues [@url:https://retractionwatch.com/retracted-coronavirus-covid-19-papers; @url:https://asapbio.org/preprints-and-covid-19].
Preprints provide a venue for scientists to release findings rapidly, but have both the advantage and disadvantage of making research available before it has undergone the peer review process.
However, some traditional publishing venues have also fast-tracked COVID-19 through peer review, leading to questions about whether this research is being held to the usual standards for publication [@doi:10.1111/bioe.12772].
Therefore, monitoring the COVID-19 literature requires not only digesting the high volume of information being released, but also critically evaluating it and/or monitoring for subsequent adjustments.

Because of the fast-moving nature of the topic, many efforts to summarize and synthesize COVID-19 literature have been undertaken. 
These efforts include newsletters [@url:https://depts.washington.edu/pandemicalliance/covid-19-literature-report/latest-reports/; @doi:10.1080/10872981.2020.1770562], web portals (such as [@url:https://outbreaksci.prereview.org/] or the now-defunct http://covidpreprints.com/, which was described in [@url:https://asapbio.org/preprints-and-covid-19]), comments on preprint servers [@doi:10.1038/s41577-020-0319-0; @url:https://disqus.com/by/sinaiimmunologyreviewproject/], and even a journal [@url:https://rapidreviewscovid19.mitpress.mit.edu/].
However, the explosive rate of publication presents challenges for such efforts, many of which are no longer publishing summaries.
Similarly, many literature reviews have been written on the available COVID-19 literature [@doi:10.1016/j.molmed.2020.02.008; @doi:10.1016/j.immuni.2020.05.002; @doi:10.1126/scitranslmed.abc1931; @doi:10.1016/j.immuni.2020.05.002; @doi:10.1001/jama.2020.6019; @doi:10.1038/d41591-020-00026-w; @doi:10.1001/jama.2020.12839].
However, static reviews quickly become outdated as new research is released or existing research is retracted or superseded
Therefore, the COVID-19 publishing climate presented a challenge where curation of the literature by a diverse group of experts in a format that could respond quickly to high-volume, high-velocity information was desirable.

We therefore sought to develop a platform for scientific discussion and collaboration around COVID-19 by adapting open publishing infrastructure to accommodate the scale of the COVID-19 publishing boom.
Recent advances in open publishing have created an infrastructure that facilitates distributed, version-controlled collaboration on manuscripts [@doi:10.1371/journal.pcbi.1007128].<!--To Do: possibly cite some other efforts here-->
Manubot [@doi:10.1371/journal.pcbi.1007128] is a collaborative framework developed to adapt open-source software development techniques and version control for manuscript writing.
With Manubot, manuscripts are managed and maintained using GitHub, a popular, online version control interface that also provides the infrastructure via continuous integration (CI) to incorporate code into the manuscript building process to allow, for example, figures to be continuously updated based on an external data set.
This open-publishing platform has been used to develop large-scale collaborative efforts such as a review of developments in deep learning [@doi:10.1098/rsif.2017.0387].
Collaboration via massively open online papers (MOOPs) has been identified as a strategy for promoting inclusion and interdisciplinary thought [@doi:10.5334/kula.63].
Manubot is an ideal platform for the analysis of COVID-19 literature because it facilitates the automatic integration of new data through CI.
However, the Manubot workflow can appear intimidating to contributors who are not well-versed in git [@doi:10.5334/kula.63].
The synthesis and discussion of the emerging literature by biomedical scientists and clinicians is imperative to a robust interpretation of COVID-19 research, but in biology, such efforts often rely on WYSIWYG tools such as Google Docs despite the significant limitations of these platforms in the face of excessive publication.
Therefore, we recognized that the problem of synthesizing the COVID-19 literature lent itself well to the Manubot platform, but that the potential technical expertise required to work with Manubot present a significant technical barrier to domain experts.

Here, we describe efforts to adapt Manubot to handle the extreme case of the COVID-19 infodemic, with the objective of extending simply reviewing preprints to develop a centralized platform for summarizing and synthesizing a massive amount of preprints, news stories, journal publications, and data.
Unlike prior collaborations built on Manubot, here most contributors came from a traditional biological or medical background.
The members of the COVID-19 Review Consortium worked to consolidate information about the virus in the context of related viruses and to synthesize rapidly emerging literature centered on the diagnosis and treatment of COVID-19.
Manubot provided the infrastructure to manage contributions from the community and create a living, scholarly document that integrated data from multiple sources to respond to the COVID-19 crisis in real time and a back-end that allowed biomedical scientists to sort and distill informative content out of the overwhelming flood of information [@doi:10.1038/s42254-020-0175-7] in order to provide a resource that would be useful to the broader scientific community.
This case study demonstrates the value of open collaborative writing tools such as Manubot to emerging challenges and the flexibility of Manubot to be adapted to problems unique to a range of fields.
By recording the evolution of information over time and assembling a resource that auto-updated in response to the evolving crisis, it revealed the particular value that Manubot holds for managing a rapid changes in scientific thought.

### METHODS

#### Contributor Recruitment

One of the primary goals of this project was to develop Manubot as a platform accessible to researchers with limited computational training, as is common in biology and medicine.<!--To Do: Halie can potentially adapt sources for this from her draft faculty apps-->
Given the limitations imposed upon scientists by the COVID-19 pandemic and social distancing measures that had most scientists (including students) working from home for much of 2020, community building across disciplines and across career stages was a priority of the project.
The current project was managed through GitHub [@url:https://github.com/greenelab/covid19-review] using Manubot [@doi:10.1371/journal.pcbi.1007128] to continuously generate a version of the manuscript online [@url:https://greenelab.github.io/covid19-review].
Contributors were recruited by word of mouth and on Twitter, and we sought out opportunities to integrate existing efforts to train early-career researchers (ECRs).
Few researchers in biological and medical fields are trained in version control tools such as git <!--To Do: Find some sort of reference for this, software carpentry? Pull from citations to 10.1371/journal.pcbi.1004668
-->

In order to make the project accessible to individuals from a number of backgrounds, we developed resources explaining how to use GitHub's web interface to develop and edit text and interact with Manubot for individuals with no prior experience working with git or other version control platforms.
<!--To Do: elaborate on resource development-->

Interested parties were encouraged to contribute in a number of ways.
One option was to submit articles of interest as issues in the GitHub repository.
Articles were classified as _diagnostic_, _therapeutic_, or _other_, and a template was developed to guide the review of papers and preprints in each category.
Following a framework often used for assessing medical literature, the review consisted of examining methods used in each relevant article, assignment (whether the study was observational or randomized), assessment, results, interpretation, and how well the study extrapolates [@doi:10.5014/ajot.60.4.367].
For examples of each template, please see Appendices B-D.
Another option was to contribute or edit text using GitHub's pull request system.
Each pull request was reviewed and approved by at least one other author.
Manubot also provides a functionality to create a bibliography using digital object identifiers (DOIs), website URLs, or other identifiers such as PubMed identifiers and arXiv IDs.

#### Applying Manubot's Existing Capabilities to the Challenges of COVID-19 

Because of the ever-evolving nature of the COVID-19 crisis, many of the figures and text proposed by subject matter contributors would have quickly become outdated.
To address this concern, Manubot and GitHub's continuous integration features were used to create figures and text that could respond to changes in the COVID-19 pandemic over time.
The combination of Manubot and GitHub Actions also made it possible to dynamically update information such as statistics and visualizations in the manuscript.
When scientific writers added text that was current only as of a given date, publicly available data sources were identified whenever possible to allow the information to pulled directly into the manuscript in order to keep it up to date.<!--To Do: Add quoted examples-->
Data was pulled from a number of sources.
Data about worldwide cases and deaths from the COVID-19 Data Repository by the Center for Systems Science and Engineering at Johns Hopkins University [@https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series] were read using a Python script. <!-- To Do: replace figure reference with manuscript reference or reference figure conditionally based on template variable [to generate Figure @fig:csse-deaths.] -->
Similarly, the clinical trials statistics and figure were generated based on data from the University of Oxford Evidence-Based Medicine Data Lab's COVID-19 TrialsTracker [@doi:10.5281/zenodo.3732709]. <!-- TODO: replace figure reference with manuscript reference or reference figure conditionally based on template variable [Figure @fig:ebm-trials] -->
In both cases, frequency data were plotted using Matplotlib [@doi:10.1109/MCSE.2007.55] in Python.
The figure showing the geographic distribution of COVID-19 clinical trials was generated using the countries associated with the trials listed in the COVID-19 TrialsTracker, converting the country names to 3-letter ISO codes using pycountry or manual adjustment when necessary, and visualizing the geographic distribution of trial recruitment using geopandas. <!-- TODO: replace figure reference with manuscript reference or reference figure conditionally based on template variable [Figure @fig:ebm-map] -->
<!--To Do: Update with Vaccines-->

GitHub Actions runs a nightly workflow to update these external data and regenerate the statistics and figures for the manuscript.
The workflow uses the GitHub API to detect and save the latest commit of the external data sources, which are both GitHub repositories.
It then downloads versioned data from that snapshot of the external repositories and runs bash and Python scripts to calculate the desired statistics and produce the summary figures.
The statistics are stored in JSON files that are accessed by Manubot to populate the values of placeholder template variables dynamically every time the manuscript is built.
For instance, the template variable {% raw %}`{{ebm_trials_results}}`{% endraw %} in the manuscript is replaced by the actual number of clinical trials with results, {{ebm_trials_results}}.
The template variables also include versioned URLs to the dynamically updated figures.
The JSON files and figures are stored in the `external-resources` branch of the manuscript's GitHub repository, which acts as versioned storage.
The GitHub Actions workflow automatically adds and commits the new JSON files and figures to the `external-resources` branch every time it runs, and Manubot uses the latest version of these resources when it builds the manuscript.

#### Updating to Manubot in Response to Project Demands

Due to the needs of this project, project contributors also implemented new features in Manubot.
<!-- To Do: Detailed description of Ryan's work making it possible to cite clinical trial identifiers-->
Manubot uses Zotero [@url:https://www.zotero.org] to extract metadata for some types of citations.
These features support directly citing clinical trial identifiers such as `clinicaltrials:NCT04292899` [@clinicaltrials:NCT04292899].

A new plugin was also added to Manubot to support "smart citations" in the HTML build of manuscripts.
The plugin uses the [Scite](https://scite.ai/) service to display a badge below any citation with a DOI.
The badge contains a set of icons and numbers that indicate how many times that source has been mentioned, supported, or disputed, and whether there have been any important editorial notices, such as retractions or corrections.
Using this, we were able to quickly identify references that needed to be checked again since the time they had been added.
This was invaluable given the nature of the project, where we were disseminating rapidly evolving information of great consequence from over a thousand different sources.
The badges also allow readers to roughly evaluate the reliability of cited sources at a glance.

Because in this implementation of Manubot, most collaborators were writing and editing text through the GitHub website rather than in a local text editor, we also needed to add spell-checking functionalities to Manubot.
<!--To Do: Describe-->

<!-- To Do: Also, if we end up automating anything to do with the Manubot-to-LaTeX or bibtex workflow while trying to submit this paper, we should describe that too, as well as the udpates for the Microsoft Word crowd to generating the complete review manuscript along with the individual manuscripts reviewing specific topics. <!-- To Do: reference individual manuscripts -->

### RESULTS

#### Recruitment and Manuscript Development

<!--To Do: Add a graph/text describing number of unique contributors & commits over time-->
<!-- To Do: Word Cloud of #17 to show diversity of interests?-->
Appendix A contains summaries written by the students, post-docs, and faculty of the Immunology Institute at the Mount Sinai School of Medicine [@url:https://github.com/ismms-himc/covid-19_sinai_reviews; @doi:10.1038/s41577-020-0319-0], and two of the authors were recruited through the American Physician Scientist Association's Virtual Summer Research Program [@url:https://www.physicianscientists.org/page/summer-research-pilot-program].
<!--To Do: Graph of contributions by individual over time and/or by career stage?-->
<!-- To Do: unique contributors and commits over time like this: https://greenelab.github.io/meta-review/#fig:contrib-->
<!--To Do: Other possible graphs: plot number of article issues opened over time, distribution of review contents, that is, something showing the number of comments per pull request that was merged, growth of the reference list over time and/or number of clinical trials cited (because we describe that as a necessary enhancement)-->

<!--To Do: Links to documentation-->

#### Data Analysis and Integration

<!--To Do: Flow chart of data integration? We could have a summary figure showing all of the external data sources that are integrated into the manuscript. We have icons for MSSM reviews, JHU data, etc. at the top. That flows into a GitHub repo, which also takes input from all the contributors. Then the output of the repo is the manuscript and other CI artifacts.-->

<!-- To Do: example of how the auto-updated figures changed between an early version and most recent version to demonstrate the importance of having current data visualized-->

The workflow file is available from <https://github.com/greenelab/covid19-review/blob/master/.github/workflows/update-external-resources.yaml> and the scripts are available from <https://github.com/greenelab/covid19-review/tree/external-resources>.
The Python package versions are available in <https://github.com/greenelab/covid19-review/blob/external-resources/environment.yml>.
<!-- To Do: These files are archived with [Zenodo?](...). -->

#### Updates to Manubot

The scite integration and spell-checking functionalities have been integrated into the current release of Manubot <!--To Do: Link-->.
Support for clinical trial identifiers is supported both by Manubot and by Zotero <!--To Do: check this phrasing-->.
Using CI, Manubot now checks that the manuscript was built correctly, runs spellchecking, and cross-references the manuscripts cited in this review, as summarized in Appendix A and discussed in the project's issues and pull requests.

### DISCUSSION

Working within the Manubot framework allowed for the successful facilitation of a massive collaborative review on an urgent topic.
Developing Manubot for the specific challenges raised by COVID-19 and expanding on both training resources resulted in seven evolving literature reviews produced by the COVID-19 Review Consortium, all of which are currently available through Manubot and, in some cases, on arXiv <!--To Do: cite individual manuscripts-->.
As many other efforts have described, the rate of publishing of formal manuscripts and preprints about COVID-19 has been unprecedented [@doi:10.1053/j.ackd.2020.08.003].
The Manubot framework will allow for continuous updating of the manuscripts as the pandemic enters its second year and the landscape shifts with the emergence of promising therapeutics and vaccines [@individual-therapeutics; @individual-vaccines].
The manuscripts pull data from XX data sources, allowing for information and visualizations to be updated daily using CI.
This computational approach allows for the information in the manuscripts to be kept up to date automatically.

Beyond the immediate goal of applying Manubot to the challenges of COVID-19, we have also expanded Manubot to allow for broader participation in open publishing from fields where computational training in tools like version control is uncommon. 
However, the broader topic of COVID-19 intersects with a wide range of fields, including virology, immunology, medicine, pharmacology, evolutionary biology, public health, and more, and any effort to comprehensively document and evaluate this body of literature would require insight from scientists across a number of fields.
Furthermore, during the initial phase of the COVID-19 pandemic during spring and summer 2020, and much longer in some part of the world, many biological scientists were unable to access their research spaces.
As a result, early career researchers (ECR) and students were likely to lose out on valuable time for conducting experiments. <!--To Do: look at equity analyses of the effects on the pandemic to see if there is any data on this yet?-->
Manubot provided a way for all contributors, including ECRs, to join a massive collaborative projects but also demonstrate their individual contributions to the larger work.
<!--To Do: summary of results of contributor analysis-->

Manubot provides the advantage of allowing a manuscript to be rendered in several formats that serve different purposes, and the current project extended these options.
For example, beyond building just a PDF, Manubot also renders the manuscript in HTML and docx <!--To Do: And tex?-->
The HTML manuscript format offers several advantages over a static PDF to harmonize available resources that we were able to apply to specific problems of COVID-19.
The integration of scite has made the expansive number of references more manageable by visually representing whether their results are contested or whether they have been corrected or retracted.
Cross-referencing cited preprints with their reviews in the appendix is another.
Docx is a necessary format for a biological collaboration where authors are typically not working in LaTeX.
<!--To Do: continue expanding to describe docx updates and possibly LaTeX updates-->

With the worldwide scientific community uniting during 2020 to investigate SARS-CoV-2 and COVID-19 from a wide range of perspectives, findings from many disciplines are relevant on a rapid timescale to a broad scientific audience.
As a result, centralizing, summarizing, and critiquing data and literature broadly relevant to COVID-19 can help to expedite the interdisciplinary scientific process that is currently happening at an advanced pace.
The efforts of the COVID-19 Review Consortium illustrate the value of including open source tools, including those focused on open publishing, in these efforts.
By facilitating the versioning of text, such platforms also allow for documentation of the evolution of thought in an evolving area.
This application of version control holds the potential to improve scientific publishing in a range of disciplines, including those outside of traditional computational fields.
