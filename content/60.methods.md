## An Open-Publishing Response to the COVID-19 Infodemic

### ABSTRACT

The COVID-19 pandemic catalyzed the rapid dissemination of a large volume of papers and preprints discussing all aspects of the virus and the disease.
Additionally, the multifaceted nature of COVID-19 demands a multidisciplinary approach, but the urgency of the crisis combined with the need for social distancing measures presents unique challenges to collaborative science.
This problem therefore represented a strong candidate for a massive online open publishing approach to collaboration using Manubot.
Through GitHub, collaborators were invited to contribute summaries and critiques of new literature via issue templates and to submit summaries of topics of interest as pull requests.
The manuscript content could be rendered from markdown into formats, including pdf, HTML, and docx, and an up-to-date version was always available online.

However, this particular project presented unique challenges that necessitated additions to Manubot.
The first set related to the technical barrier to entry, as most contributors were from biomedical backgrounds and had limited experience with git.
We developed training resources to allow for participation through GitHub's web interface, added support for the citation of clinical trial identifiers, and added a spell-checking feature to Manubot.
We also expanded the range of formats and options available for export.
The other category of challenges arose because of the extreme volume and velocity of COVID-19 publication.
Manubot already generated in-text figures from code, but for this project, we adapted this workflow to allow for the retrieval of up-to-date data from online sources nightly.
Additionally, we integrated scite, a tool for checking the status of references including retractions, into the HTML build in order to simplify the process of monitoring changes to publications after their release. 

Through this effort, we organized over 50 scientists from a range of backgrounds who evaluated over 1000 sources and authored seven literature reviews that are currently in submission or in preparation for submission.
This project illustrates that Manubot is an adaptable workflow that can handle even an extreme volume and velocity of information, and that its back-end technical complexity does not prohibit the inclusion of non-technical contributors.
While many efforts from the computational community have focused on mining COVID-19 literature, this implementation illustrates the power of open publishing to organize people to aggregate and disseminate information in response to an evolving crisis.
Applying a massive online open publishing approach to this problem allowed us to develop a community that occupies a unique niche in the COVID-19 research space.

### CCS CONCEPTS

<!--To Do: @mprobson, do you want to take the lead on this?-->

### KEYWORDS

### INTRODUCTION

Coronavirus Disease 2019 (COVID-19) has shaped the years 2020 and 2021 by causing a world-wide public health crisis.
The scientific community has responded by turning significant attention and resources towards COVID-19 and the associated virus, SARS-CoV-2.
The result has been the rapid release of data, results, and publications related to COVID-19 at a scale never previously seen.
Over 20,000 articles about COVID-19 were released in the first 4 months of the pandemic [@doi:10.1053/j.ackd.2020.08.003], and the velocity and volume of information being released led to the pandemic being termed as an "infodemic" as well [@doi:10.1053/j.ackd.2020.08.003; @doi:10/ggpx67].
While this influx of information is likely evidence of important work towards understanding the virus and the disease, there are also downsides to the availability of too much information.
The potential for "excessive publication" has been identified as an issue for over forty years, and was one concern about the move towards electronic, rather than print, publishing at the turn of the millennium [@doi:10/d3bmnv].
The contents of the COVID-19 Open Research Dataset (CORD-19) [@arxiv:2004.10706], which was developed in part to assist in efforts to train machine learning algorithms on COVID-19-related text, illustrates the volume of publication relevant to understanding this virus (Figure @fig:cord19-growth).
This resource was developed by querying several sources for terms related to SARS-CoV-2 and COVID-19, as well as the coronaviruses SARS-CoV-1 and MERS-CoV and their associated viruses [@arxiv:2004.10706] and contains {{cord19_total_pubs}} manuscripts as of {{cord19_date_pretty}}.
Additional curation by CoronaCentral [@doi:10.1101/2020.12.21.423860] has produced, at present, a set of over 150,000 publications particularly relevant to COVID-19 and these closely related viruses.
Thus, any effort to synthesize, summarize, and contextualize COVID-19 research will be faced with a vast corpus of potentially relevant material.

**Change over time in the number of publications in the CORD-19 dataset.**
As of {{cord19_date_pretty}}, there are {{cord19_total_pubs}} in the CORD-19 dataset.
The first release, on March 16, 2020, contained 28,000 manuscripts on topics relevant to SARS-CoV-2 and related coronaviruses [@arxiv:2004.10706].
Since then, the resource has continued to grow rapidly (left), with both traditionally published and preprint manuscripts in the corpus (right).
At present, it contains {{cord19_total_preprints}} preprints from _arXiv_, _bioRxiv_, and _medRxiv_.
While not all of the manuscripts are specifically focused on SARS-CoV-2 or COVID-19, this corpus is likely to contain all or most manuscripts that would be explored in developing a literature review, which requires assessing both emerging and prior research.
]({{cord19_figure}} "CORD-19 dataset growth"){#fig:cord19-growth secno=1}
<!--To Do: Possible viz of change in clinical trials as well?-->

With information being produced rapidly through both traditional publishing venues and preprint servers, some papers that are published face scrutiny after their initial release.
Concerns have been raised that the number of COVID-19 papers being retracted may be higher, or potentially much higher, than is typical, although a thorough investigation of this question will not be possible until more time has elapsed [@doi:10.1080/08989621.2020.1782203; @doi:10.1080/08989621.2020.1793675].
Other papers are updated with corrections or expressions of concern [@doi:10.1080/08989621.2020.1793675; @url:https://retractionwatch.com/retracted-coronavirus-covid-19-papers].
These include both preprints and papers that are published in more traditional venues [@url:https://retractionwatch.com/retracted-coronavirus-covid-19-papers; @url:https://asapbio.org/preprints-and-covid-19].
Preprints provide a venue for scientists to release findings rapidly, but have both the advantage and disadvantage of making research available before it has undergone the peer review process.
However, some traditional publishing venues have also fast-tracked COVID-19 through peer review, leading to questions about whether this research is being held to the usual standards for publication [@doi:10.1111/bioe.12772].
Therefore, monitoring the COVID-19 literature requires not only digesting the high volume of information being released, but also critically evaluating it and/or monitoring for subsequent adjustments.

Because of the fast-moving nature of the topic, many efforts to summarize and synthesize COVID-19 literature have been undertaken. 
These efforts include newsletters [@url:https://depts.washington.edu/pandemicalliance/covid-19-literature-report/latest-reports/; @doi:10.1080/10872981.2020.1770562], web portals (such as [@url:https://outbreaksci.prereview.org; @doi:10.1126/science.abc7839] or the now-defunct http://covidpreprints.com/, which was described in [@url:https://asapbio.org/preprints-and-covid-19]), comments on preprint servers [@doi:10.1038/s41577-020-0319-0; @url:https://disqus.com/by/sinaiimmunologyreviewproject/], and even a journal [@url:https://rapidreviewscovid19.mitpress.mit.edu/].
However, the explosive rate of publication presents challenges for such efforts, many of which are no longer publishing summaries.
Similarly, many literature reviews have been written on the available COVID-19 literature [@doi:10.1016/j.molmed.2020.02.008; @doi:10.1016/j.immuni.2020.05.002; @doi:10.1126/scitranslmed.abc1931; @doi:10.1016/j.immuni.2020.05.002; @doi:10.1001/jama.2020.6019; @doi:10.1038/d41591-020-00026-w; @doi:10.1001/jama.2020.12839].
However, static reviews quickly become outdated as new research is released or existing research is retracted or superseded; one example is a review that reviewed topics in COVID-19 research including vaccine development [@doi:10.1001/jama.2020.12839].
This review was published on July 10, 2020, four days before Moderna released the surprisingly promising results of their phase 1 trial [@doi:10.1056/NEJMoa2022483] that changed expectations surrounding vaccines.
Therefore, the COVID-19 publishing climate presented a challenge where curation of the literature by a diverse group of experts in a format that could respond quickly to high-volume, high-velocity information was desirable.

We therefore sought to develop a platform for scientific discussion and collaboration around COVID-19 by adapting open publishing infrastructure to accommodate the scale of the COVID-19 publishing boom.
Recent advances in open publishing have created an infrastructure that facilitates distributed, version-controlled collaboration on manuscripts [@doi:10.1371/journal.pcbi.1007128].<!--To Do: possibly cite some other efforts here-->
Manubot [@doi:10.1371/journal.pcbi.1007128] is a collaborative framework developed to adapt open-source software development techniques and version control for manuscript writing.
With Manubot, manuscripts are managed and maintained using GitHub, a popular, online version control interface that also provides the infrastructure via continuous integration (CI) to incorporate code into the manuscript building process to allow, for example, figures to be continuously updated based on an external data set.
This open-publishing platform has been used to develop large-scale collaborative efforts such as a review of developments in deep learning [@doi:10.1098/rsif.2017.0387] and a re-evaluation of the role of authorship in modern collaborations [@doi:10.1080/08989621.2020.1779591].
Collaboration via massively open online papers has been identified as a strategy for promoting inclusion and interdisciplinary thought [@doi:10.5334/kula.63].
Manubot is an ideal platform for the analysis of COVID-19 literature because it facilitates the automatic integration of new data through CI.
However, the Manubot workflow can appear intimidating to contributors who are not well-versed in git [@doi:10.5334/kula.63].
The synthesis and discussion of the emerging literature by biomedical scientists and clinicians is imperative to a robust interpretation of COVID-19 research, but in biology, such efforts often rely on What You See Is What You Get tools such as Google Docs despite the significant limitations of these platforms in the face of excessive publication.
Therefore, we recognized that the problem of synthesizing the COVID-19 literature lent itself well to the Manubot platform, but that the potential technical expertise required to work with Manubot present a significant technical barrier to domain experts.

Here, we describe efforts to adapt Manubot to handle the extreme case of the COVID-19 infodemic, with the objective of extending simply reviewing preprints to develop a centralized platform for summarizing and synthesizing a massive amount of preprints, news stories, journal publications, and data.
Unlike prior collaborations built on Manubot, here most contributors came from a traditional biological or medical background.
The members of the COVID-19 Review Consortium worked to consolidate information about the virus in the context of related viruses and to synthesize rapidly emerging literature centered on the diagnosis and treatment of COVID-19.
Manubot provided the infrastructure to manage contributions from the community and create a living, scholarly document that integrated data from multiple sources to respond to the COVID-19 crisis in real time and a back-end that allowed biomedical scientists to sort and distill informative content out of the overwhelming flood of information [@doi:10.1038/s42254-020-0175-7] in order to provide a resource that would be useful to the broader scientific community.
This case study demonstrates the value of open collaborative writing tools such as Manubot to emerging challenges and the flexibility of Manubot to be adapted to problems unique to a range of fields.
By recording the evolution of information over time and assembling a resource that auto-updated in response to the evolving crisis, it revealed the particular value that Manubot holds for managing a rapid changes in scientific thought.

### METHODS

#### Contributor Recruitment and Roles

A preliminary requirement for this undertaking was to establish Manubot as a platform accessible to researchers with limited experience working with git, as is common in biology and medicine, where version control is not typically emphasized [@doi:10.1109/SE4Science.2017.11; @doi:10.1177/2515245918754826; @doi:10.1186/1751-0473-8-7].
Contributors were recruited by word of mouth and on Twitter, and we sought out opportunities to integrate existing efforts to train early-career researchers (ECRs).
We invited potential collaborators to contribute a short introduction on a GitHub issue in order to collect information about who was involved and provide an introduction to working with GitHub issues.
Interested participants were encouraged to contribute in a number of ways.
One option was to catalog articles of interest as issues in the GitHub repository.
We developed a standardized set of questions for contributors to consider when evaluating an article following a framework often used for assessing medical literature.
This approach emphasizes examining the methods used, assignment (whether the study was observational or randomized), assessment, results, interpretation, and how well the study extrapolates [@doi:10.5014/ajot.60.4.367].
Contributors were also invited to contribute or edit text using GitHub's pull request system.
These contributions were not strictly defined, and could range from minor corrections to punctuation and grammar to large-scale additions of text.
Each pull request was reviewed and approved by at least one other contributor before being merged into the main branch.
We sought to tag potential reviewers based on the introductions they had contributed in order to encourage participation.
The emphasis on issues and pull requests was designed to encourage authors to discuss papers in the issues and to provide feedback (both formal and informal) on proposed text additions or changes.
We also used gitter [@url:https://gitter.im] to promote informal questions and sharing of information among collaborators.

#### Utilization and Expansion of Manubot

Applying Manubot's existing capabilities allowed us to confront a number of standard challenges in large-scale collaborations, such as the maintenance of a record of contributions that allowed us to allocate credit appropriately or to contact the original author if questions arose.
Additionally, an up-to-date version of the content was available at all times online at https://greenelab.github.io/covid19-review/.
This approach also allowed us to minimize the demand on authors to curate bibliographic resources.
Manubot provides the functionality to create a bibliography using digital object identifiers (DOIs), website URLs, or other identifiers such as PubMed identifiers and arXiv IDs.
The author can insert a citation in-line using a format such as {% raw %}`[@doi:10.1371/journal.pcbi.1007128]`{% endraw %}.
Manubot then obtains reference metadata, exports the citations as Citation Style Language JSON Data Items, which is an open standard, and renders the bibliographic information needed to generate the references section [@doi:10.1371/journal.pcbi.1007128].
This approach allows multiple authors to work on a piece of text without needing to manually make adjustments to the reference lists.

Due to the needs of this project, several new features were also implemented in Manubot.
Because of the ever-evolving nature of the COVID-19 crisis, many of the figures and text proposed by subject matter contributors would have quickly become outdated.
To address this concern, Manubot and GitHub's CI features were used to create figures that integrated online data sources to respond to changes in the COVID-19 pandemic over time.
The combination of Manubot and GitHub Actions also made it possible to dynamically update information, such as the current number of active COVID-19 clinical trials [@individual-therapeutics], within the text of the manuscripts.
GitHub Actions runs a nightly workflow to update these external data and regenerate the statistics and figures for the manuscript.
The workflow uses the GitHub API to detect and save the latest commit of the external data sources, which are both GitHub repositories.
It then downloads versioned data from that snapshot of the external repositories and runs bash and Python scripts to calculate the desired statistics and produce the summary figures.
The statistics are stored in JSON files that are accessed by Manubot to populate the values of placeholder template variables dynamically every time the manuscript is built.
For instance, the template variable {% raw %}`{{ebm_trials_results}}`{% endraw %} in the manuscript is replaced by the actual number of clinical trials with results, {{ebm_trials_results}}.
The template variables also include versioned URLs to the dynamically updated figures.
The JSON files and figures are stored in the `external-resources` branch of the manuscript's GitHub repository, which acts as versioned storage.
The GitHub Actions workflow automatically adds and commits the new JSON files and figures to the `external-resources` branch every time it runs, and Manubot uses the latest version of these resources when it builds the manuscript.
For figures, data are then plotted using Matplotlib [@doi:10.1109/MCSE.2007.55] in Python and pushed to a branch where they can be read into the main manuscript.
The workflow file is available from <https://github.com/greenelab/covid19-review/blob/master/.github/workflows/update-external-resources.yaml> and the scripts are available from <https://github.com/greenelab/covid19-review/tree/external-resources>.
The Python package versions are available in <https://github.com/greenelab/covid19-review/blob/external-resources/environment.yml>.<!-- To Do: These files are archived with [Zenodo?](...). -->

Another issue that emerged was the need for a standardized way to cite clinical trials.
Clinical trials that are registered with clinicaltrials.gov [@url:https://clinicaltrials.gov] receive a unique clinical trial identifier.
Because clinical trials are registered long before results are available in manuscript form, it was important to this project to be able to refer to the clinical trial identifiers associated with a large number of relevant trials (Figure @fig:ebm-trials).
Manubot uses Zotero [@url:https://www.zotero.org] to extract metadata for some types of citations, but Zotero did not support clinical trial identifiers.
In order to enable Manubot to pull metadata associated with clinical trials based on their identifiers, we needed to develop Zotero support for these identifiers.
The need for this feature to be enabled in Zotero had also been identified by other researchers [@url:https://forums.zotero.org/discussion/74933/import-from-clinical-trials-registry; @url:https://forums.zotero.org/discussion/77721/add-reference-from-clinical-trials-org].
This update was achieved through the implementation of a query of clinicaltrials.gov to retrieve XML metadata associated with each identifier using JavaScript [@url:https://github.com/zotero/translators/pull/2153]. 

Another challenge that emerged was that, because of the large number of citations used in this manuscript and the fast-moving nature of COVID-19 research, keeping track of retractions, corrections, and notices of concern became a priority.
A new plugin was added to Manubot to support "smart citations" in the HTML build of manuscripts.
The plugin uses the [Scite](https://scite.ai/) service to display a badge below any citation with a DOI.
The badge contains a set of icons and numbers that indicate how many times that source has been mentioned, supported, or disputed, and whether there have been any important editorial notices, such as retractions or corrections.
Using this, we were able to quickly identify references that needed to be checked again since the time they had been added.
This was invaluable given the nature of the project, where we were disseminating rapidly evolving information of great consequence from over a thousand different sources.
The badges also allow readers to roughly evaluate the reliability of cited sources at a glance.

Because in this implementation of Manubot, most collaborators were writing and editing text through the GitHub website rather than in a local text editor, we also needed to add spell-checking functionalities to Manubot.
<!--To Do: Describe -- perhaps @agitter can lead this?-->

Manubot provides the advantage of allowing a manuscript to be rendered in several formats that serve different purposes, and the current project extended these options.
Prior to this project, Manubot was able to convert the markdown-formatted manuscript to HTML, pdf, and docx formats.
For the current project, we expanded this functionality to allow for individual sections of the manuscript to be rendered as separate docx files.
This development was necessary because the manuscript grew so large that it needed to be split into seven separate papers for submission.
Similarly, we expanded the range of possible export formats to include LaTeX.
Because LaTeX is used for manuscript submission in many fields, automating the process of converting markdown to a submission-friendly format is desirable. 
<!-- To Do: Also, if we end up automating anything to do with the Manubot-to-LaTeX or bibtex workflow while trying to submit this paper, we should describe that too, as well as the udpates for the Microsoft Word crowd to generating the complete review manuscript along with the individual manuscripts reviewing specific topics.

### RESULTS

#### Recruitment and Manuscript Development

![
**Visualization of collaborator interests as expressed in the introductory issue**
Potential collaborators were invited to practice commenting on an issue by sharing their academic backgrounds and interest in the project.
These responses were visualized using wordcloud2 in R (https://CRAN.R-project.org/package=wordcloud2).
Common respones included terms like contribute (44 uses), biology (42 uses), data (38 uses), help (36 uses), learn (32 uses), experience (31 uses), and student (25 use).
]((images/interests.png)("Wordcloud of contributor interests"){#fig:wordcloud secno=1}

We received a large amount of interest from the broad community beginning with coverage of the project by _Nature Toolbox_ [@doi:10.1038/d41586-020-00916-6] and an associated tweet about the project on April 1, 2020 [@url:https://twitter.com/j_perkel/status/1245454628235309057].
Interested participants came from a wide range of backgrounds and many of the responses emphasized a willingness to learn about a new topic, as well asn an interest in COVID-19 and SARS-CoV-2 (Figure @fig:wordcloud).
"Biology" was one of the most commonly emphasized interests, with almost double the number of uses as "computational."
This pattern suggests that, as anticipated, we primarily recruited researchers from traditional biological backgrounds.
Because the GitHub issues and comment systems are relatively similar to other web common web activities, we found that authors were able to learn these tools fairly quickly.
Similarly, the gitter chat also presented a low barrier to entry.
While only a fraction of potential contributors ended up contributing to the text included in the manuscripts, many of these contributors remained engaged over the course of a full year (Figure with dots).

(Dot plot of contributions will go here)

In order to make the project accessible to individuals from a number of backgrounds, we developed resources explaining how to use GitHub's web interface to develop and edit text and interact with Manubot for individuals with no prior experience working with git or other version control platforms.
We developed tutorials containing visuals to explain how to open an issue, open a pull request, and review a pull request [@url:https://github.com/greenelab/covid19-review/blob/master/CONTRIBUTING.md; @url:https://github.com/greenelab/covid19-review/blob/master/INSTRUCTIONS.md].
Additionally, the framework for evaluating literature was converted into issue templates to simplify review of new articles.
Articles were classified as _diagnostic_ [@url:https://github.com/greenelab/covid19-review/blob/master/.github/ISSUE_TEMPLATE/new-diag-study-template.md], _therapeutic_ [@url:https://github.com/greenelab/covid19-review/blob/master/.github/ISSUE_TEMPLATE/new-therapy-study-template.md], or _other_ [@url:https://github.com/greenelab/covid19-review/blob/master/.github/ISSUE_TEMPLATE/new-paper-template.md], with an associated template developed to guide the review of papers and preprints in each category.
<!-- To Do: Number of paper issue and number of unique contributors?-->

Currently, three manuscripts associated with this project have been submitted to _mSystems_, with one accepted for publication [@individual-nutraceuticals] and two under revision [@individual-pathogenesis; @individual-pharmaceuticals].
An additional four manuscripts are in preparation.
All seven will be submitted to _mSystems_ as part of a special issue that is providing support for evolving reviews, so that they can continue to be updated as more information becomes available.
These manuscripts cover a wide range of topics including the fundamental biology of SARS-CoV-2 (pathogenesis [@individual-pathogenesis] and evolution [@individual-evolution]), biomedical advances in responding to the virus and COVID-19 (pharmaceutical therapeutics [@individual-pharmaceuticals], nutraceutical therapeutics [@individual-nutraceuticals], vaccines [@individual-vaccines], and diagnostic technologies [@individual-diagnostics]), and biological and social factors influencing variable effects on patients [@individual-inequality].<!--To Do: add these additional individual citations-->
To date, [number] authors are associated with the consortium.<!--To Do: break down by undergrad, grad/med student, post-doc, jr faculty?-->
Efforts to integrate with existing projects providing support for undergraduate students during COVID-19 were also successful.
Appendix A contains summaries written by the students, post-docs, and faculty of the Immunology Institute at the Mount Sinai School of Medicine [@url:https://github.com/ismms-himc/covid-19_sinai_reviews; @doi:10.1038/s41577-020-0319-0].
Additionally, two of the consortium authors were undergraduate students recruited through the American Physician Scientist Association's Virtual Summer Research Program [@url:https://www.physicianscientists.org/page/summer-research-pilot-program].
Thus, the consortium is expected to be successful in providing a venue for researchers across all career stages to continue investigating and publishing at a time when many biomedical researchers were unable to access their laboratory facilities.

#### Using Manubot to Investigate COVID-19

Data were integrated into the manuscripts from a number of sources.
Data about worldwide cases and deaths from the COVID-19 Data Repository by the Center for Systems Science and Engineering at Johns Hopkins University [@https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series] were read using a Python script. <!-- To Do: replace figure reference with manuscript reference or reference figure conditionally based on template variable [to generate Figure @fig:csse-deaths.] -->
Similarly, the clinical trials statistics and figure were generated based on data from the University of Oxford Evidence-Based Medicine Data Lab's COVID-19 TrialsTracker [@doi:10.5281/zenodo.3732709]. <!-- TODO: replace figure reference with manuscript reference or reference figure conditionally based on template variable [Figure @fig:ebm-trials] -->
The evolution of this figure over time is shown in Figure @fig:ebm-trials.
Information about vaccine distribution was extracted from Our World In Data [@url:https://github.com/owid/covid-19-data].
Figure 1 (above) also uses this approach to dynamically integrate data directly from the CORD-19 dataset [@arxiv:2004.10706].<!--To Do: Flow chart of data integration? We could have a summary figure showing all of the external data sources that are integrated into the manuscript. We have icons for MSSM reviews, JHU data, etc. at the top. That flows into a GitHub repo, which also takes input from all the contributors. Then the output of the repo is the manuscript and other CI artifacts.-->

![
**Change in the COVID-19 clinical trials figure over time.**
When we first produced this figure on July 7, 2020, there were 3,733 clinial trials in the University of Oxford Evidence-Based Medicine Data Lab's COVID-19 TrialsTracker [@doi:10.5281/zenodo.3732709].
As of {{ebm_date_pretty}}, it contains {{ebm_all_trials}}.
We were also able to easily reconfigure the figure prior to journal submission to emphasize interventional trials based on the recommendation of a collaborator who is a clinician.
This figure is included in an analysis of pharmaceutical development efforts during COVID-19 [@individual-pharmaceuticals].
]((images/ebmdatalab-trials-original.png){{ebm_trials_figure}} "COVID-19 clinical trials"){#fig:ebm-trials secno=1}

<!--To Do: Discussion of the absurd number of references?-->

The new features required for the COVID-19 project are now included in Manubot's rootstock.
Manubot and Zotero now support citing clinical trial identifiers such as `clinicaltrials:NCT04292899` [@clinicaltrials:NCT04292899], and the scite integration and spell-checking functionalities have been integrated into the current release of Manubot <!--To Do: Link-->.
<!--To Do: have the individual builds been pushed up to rootstock?-->
Using CI, Manubot now checks that the manuscript was built correctly, runs spellchecking, and cross-references the manuscripts cited in this review. <!-- we probably can't include the appendix? original sentence ended with: as summarized in Appendix A and discussed in the project's issues and pull requests.-->

### DISCUSSION

The current project was managed through GitHub [@url:https://github.com/greenelab/covid19-review] using Manubot [@doi:10.1371/journal.pcbi.1007128] to continuously generate a version of the manuscript online [@url:https://greenelab.github.io/covid19-review].
The Manubot framework facilitated  a massive collaborative review on an urgent topic.
This project demonstrates that Manubot can be applied to projects where not all collaborators have expertise or even experience working with version control pipelines.
The development of cyberinfrastructure both for training novice users to interact with GitHub and to simplify the workflows to allow them to receive many of the benefits of What You See Is What You Get platforms such as Google Docs, we were able to adapt a powerful open publishing tool to harness the domain expertise of a large group of non-technical users and to respond to the flood of COVID-19 publications.

While Manubot manuscripts are written in markdown, they can be rendered in a number of formats that provide different advantages.
For example, beyond building just a PDF, Manubot also renders the manuscript in HTML, docx, and now, LaTeX.
The HTML manuscript format offers several advantages over a static PDF to harmonize available resources that we were able to apply to specific problems of COVID-19.
The integration of scite into the HTML build makes references more manageable by visually representing whether their results are contested or whether they have been corrected or retracted.
Cross-referencing different pieces of the manuscript, such as cited preprints with reviews stored in an appendix, is another dynamic option presented by HTML.
Additionally, because of the heavy emphasis on Word processing in biology, Manubot's ability to generate docx was expanded to allow users to generate docx files containing only a section of the manuscript.
In our case, where the full project is nearly 100,000 words, this allows individual pieces to be shared widely.
Finally, the addition of LaTeX output is useful for researchers from computational fields who submit papers in tex format and removes the step of reformatting markdown prior to submission. 

Working with biomedical scientists not only addressed the immediate goal of applying Manubot to the challenges of COVID-19, but also provided a second but equally important outcome.
The COVID-19 Review Consortium provided a platform for researchers to engage in scientific investigation during the initial phase of the COVID-19 pandemic during 2020 at a time when many biological scientists were unable to access their research spaces.
In turn, by seeking to adapt Manubot to allow for broader participation in open publishing from fields where computational training in tools like version control is uncommon, we made a number of improvements that will make it more appealing to researchers from all backgrounds.
Manubot provided a way for all contributors, including ECRs, to join a massive collaborative projects but also demonstrate their individual contributions to the larger work and to gain experience with version control.
This project demonstrates that massive online open publishing efforts can indeed advance scholarship through inclusion [@doi:10.5334/kula.63], including during the extreme challenges presented by the COVID-19 pandemic.

With the worldwide scientific community uniting during 2020 and 2021 to investigate SARS-CoV-2 and COVID-19 from a wide range of perspectives, findings from many disciplines are relevant on a rapid timescale to a broad scientific audience.
As many other efforts have described, the rate of publishing of formal manuscripts and preprints about COVID-19 has been unprecedented [@doi:10.1053/j.ackd.2020.08.003], and efforts to review the body of COVID-19 literature are faced with an ever-expanding corpus to work from.
In the case of the seven manuscripts produced by the COVID-19 Review Consortium, Manubot will allow for continuous updating of the manuscripts as the pandemic enters its second year and the landscape shifts with the emergence of promising therapeutics and vaccines [@individual-therapeutics; @individual-vaccines].
These manuscripts pull data from XX data sources, allowing for information and visualizations to be updated daily using CI.
This computational approach allows for some of the updating process to be off-loaded so that domain experts can focus on the broader implications of new information as it emerges.
As a result, centralizing, summarizing, and critiquing data and literature broadly relevant to COVID-19 can help to expedite the interdisciplinary scientific process that is currently happening at an advanced pace.
The efforts of the COVID-19 Review Consortium illustrate the value of including open source tools, including those focused on open publishing, in these efforts.
By facilitating the versioning of text, such platforms also allow for documentation of the evolution of thought in an evolving area.
This application of version control holds the potential to improve scientific publishing in a range of disciplines, including those outside of traditional computational fields.
While Manubot is a technically complex tool, this project demonstrates that it can be broadly appealing even outside of technical areas of research.
